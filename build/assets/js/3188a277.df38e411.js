"use strict";(self.webpackChunkportal_2=self.webpackChunkportal_2||[]).push([[778],{3905:function(e,t,n){n.d(t,{Zo:function(){return p},kt:function(){return h}});var a=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var s=a.createContext({}),c=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},p=function(e){var t=c(e.components);return a.createElement(s.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,o=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),d=c(n),h=i,u=d["".concat(s,".").concat(h)]||d[h]||m[h]||o;return n?a.createElement(u,r(r({ref:t},p),{},{components:n})):a.createElement(u,r({ref:t},p))}));function h(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=n.length,r=new Array(o);r[0]=d;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:i,r[1]=l;for(var c=2;c<o;c++)r[c]=n[c];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},3531:function(e,t,n){n.r(t),n.d(t,{assets:function(){return p},contentTitle:function(){return s},default:function(){return h},frontMatter:function(){return l},metadata:function(){return c},toc:function(){return m}});var a=n(7462),i=n(3366),o=(n(7294),n(3905)),r=["components"],l={sidebar_position:1},s=void 0,c={unversionedId:"WritingSamples/Speed-Detection",id:"WritingSamples/Speed-Detection",title:"Speed-Detection",description:"Project Overview",source:"@site/docs/WritingSamples/Speed-Detection.md",sourceDirName:"WritingSamples",slug:"/WritingSamples/Speed-Detection",permalink:"/WritingSamples/Speed-Detection",editUrl:"https://github.com/lvllvl/portal2.git/docs/WritingSamples/Speed-Detection.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Project List",permalink:"/projectList"},next:{title:"Leetcode Solution",permalink:"/WritingSamples/Leetcode Solution"}},p={},m=[{value:"Project Overview",id:"project-overview",level:2},{value:"Project Workflow",id:"project-workflow",level:4},{value:"Preparing the dataset",id:"preparing-the-dataset",level:2},{value:"<ins>Segmentation</ins>",id:"segmentation",level:4},{value:"<ins>Optical Flow</ins>",id:"optical-flow",level:4},{value:"<ins>One Hot Encoding</ins>",id:"one-hot-encoding",level:4},{value:"<ins>Combining One Hot with Optical Flow</ins>",id:"combining-one-hot-with-optical-flow",level:4}],d={toc:m};function h(e){var t=e.components,n=(0,i.Z)(e,r);return(0,o.kt)("wrapper",(0,a.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h2",{id:"project-overview"},"Project Overview"),(0,o.kt)("h4",{id:"project-workflow"},"Project Workflow"),(0,o.kt)("p",null,"We will begin with a video in ",(0,o.kt)("inlineCode",{parentName:"p"},"mp4")," format. The video contains dash cam footage\nof a vehicle driving in downtown San Francisco. Our goal is to predict the\nspeed of the car at each frame ( 20, 400 total )."),(0,o.kt)("p",null,"Our workflow will be as follows: "),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"- Video -> 20,400 images -> Segmentation\n- Segmentation -> optical flow matrix set \n- Segmentation -> one hot encoding matrix set \n- Combine one hot encoding with optical flow \n- Long Short Term Memory ( LSTM ) model \n")),(0,o.kt)("h2",{id:"preparing-the-dataset"},"Preparing the dataset"),(0,o.kt)("h4",{id:"segmentation"},(0,o.kt)("ins",null,"Segmentation")),(0,o.kt)("p",null,"After separating the video into 20,400 images we will process each image through segmentation."),(0,o.kt)("p",null,"Segmentation is a form of image classification. A typical image classifier has\na set number of classes. For example we can make a binary classifier that\ndistinguishes between Adam Sandler or Brad Pitt. Our classifier will label an image as Adam Sandler if it thinks the entire image resembles Adam Sandler. Likewise with Brad Pitt. "),(0,o.kt)("p",{align:"center"},(0,o.kt)("img",{width:"460",height:"300",src:"../../images/AdamBradTogether.jpg"})),(0,o.kt)("p",null,"But what if the image contains both Brad Pitt ",(0,o.kt)("em",{parentName:"p"},"and")," Adam Sandler? How should the\nimage classifier categorize the image? Segmentation is a way to work around\nthis problem. This technique anticipates that both our classes ( Brad Pitt and\nAdam Sandler ) may be in an image at the same time and therefore approaches\nclassification differently. Instead of classifying an ",(0,o.kt)("em",{parentName:"p"},"entire")," image as Brad or Adam,\nsegmentation classifies each pixel as either Adam or Brad. "),(0,o.kt)("p",{align:"center"},(0,o.kt)("img",{width:"460",height:"300",src:"../../images/SegmentationFastai.png"})),(0,o.kt)("p",null,"So taking the information we gleaned from the Brad and Adam example, let's apply it to our current image set. We have driving footage so we need to figure out a way to categorize each object that appears in our images. ",(0,o.kt)("a",{parentName:"p",href:"https://medium.com/analytics-vidhya/image-segmentation-using-fastai-ddded25f811e"},"Fast.Ai")," created a segmentation model with the following 31 classes below: "),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"classes = [ 'Animal', 'Archway', 'Bicyclist', 'Building', 'Car', 'CartLuggagePram', 'Child', 'Column_Pole', 'Fence', 'LaneMkgsDriv',\n'LaneMkgsNonDriv', 'Misc_Text', 'MotorcycleScooter', 'OtherMoving', 'ParkingBlock', 'Pedestrian', 'Road', 'RoadShoulder', 'Sidewalk', \n'SignSymbol', 'Sky', 'SUVPickupTruck', 'TrafficCone', 'TrafficLight', 'Train', 'Tree', 'Truck_bus', 'Tunnel', 'VegetagtionMisc', 'Void', \n'Wall' ] \n")),(0,o.kt)("p",null,"Once the image is segmented each of the 31 classes are represented in two ways. If the image is in matrix format the class is shown as a number ( e.g., 0 - 30 ), and in image format we differentiate classes by color ( e.g., sidewalks are purple, buildings are orange ). "),(0,o.kt)("p",null,"Segmenting all our images will allow us to implement other techniques that will\nhelp us detect the speed at each frame. From this point we will create two sets of our segmentation image set. One for our optical flow processing and the other for our one hot encoding processing. "),(0,o.kt)("h4",{id:"optical-flow"},(0,o.kt)("ins",null,"Optical Flow")),(0,o.kt)("table",null,(0,o.kt)("tr",null,(0,o.kt)("th",null," dash cam footage "),(0,o.kt)("th",null," optical flow example ")),(0,o.kt)("tr",null,(0,o.kt)("td",null," ",(0,o.kt)("img",{src:"../../images/dash_cam.gif"})),(0,o.kt)("td",null,(0,o.kt)("img",{src:"../../images/optical_flow_2.gif"})))),(0,o.kt)("p",null,"Optical flow is a computer vision tool used for sequential data. In our case\nwe're using it to track any movement from image to image. This allows us to see\nwhich of our classes ( pedestrians, street signs ) are moving through\nour sequence of images. "),(0,o.kt)("p",null,"Let's imagine again our image as a matrix. Our matrix has dimensions of ",(0,o.kt)("strong",{parentName:"p"},"20 x\n30"),". Optical flow uses small filters ( i.e., a smaller matrix, usually ",(0,o.kt)("strong",{parentName:"p"},"3 x 3")," or\n",(0,o.kt)("strong",{parentName:"p"},"5 x 5")," ) that hovers over every part of the ",(0,o.kt)("strong",{parentName:"p"},"20 x 30")," matrix and identifies\nsmall dense clusters of similar pixels and then compares it to the next\nmatrix in the sequence. "),(0,o.kt)("p",null,"Visually this ends up looking like the image above on the right, only movement is\ntracked. Comparing the optical flow gif to the video frame gif on the left we\ncan see that lane markings are being picked up by the optical flow model!\nThat's great news for us because we want to give our LSTM model an object to\nreference so that it can develop a speed estimate for each frame. "),(0,o.kt)("p",null,"For more information and an implementation of optical flow check ",(0,o.kt)("a",{parentName:"p",href:"https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_video/py_lucas_kanade/py_lucas_kanade.html"},"this")," out."),(0,o.kt)("h4",{id:"one-hot-encoding"},(0,o.kt)("ins",null,"One Hot Encoding")),(0,o.kt)("p",{align:"center"},(0,o.kt)("img",{width:"460",height:"300",src:"../../images/SegmentationFastai.png"})),(0,o.kt)("p",null,"Recall our segmented image example, we have 31 classes total. Our original video is 17 minutes of driving. All 31 classes are not always present in the frame. For example sometimes there are buildings present and other times there are not. "),(0,o.kt)("p",null,"There is another way of communicating that fact. In matrix form an image may be size ",(0,o.kt)("strong",{parentName:"p"},"1 x W x H")," where ",(0,o.kt)("strong",{parentName:"p"},"W")," = width, ",(0,o.kt)("strong",{parentName:"p"},"H")," = height, and 1 = the number of layers in the matrix. Each pixel is labeled for its corresponding class, but how do we know if a class is not present in the image?"),(0,o.kt)("p",null,"One hot encoding helps display that information. Instead of representing our classes as individual pixels, each class is represented as an individual layer. If the class is not present in the image, the entire layer is filled with 0's. If the class is present, there are 1's wherever the class object is located and 0's everywhere else. In the one hot encoding matrix our new dimensions are now ",(0,o.kt)("strong",{parentName:"p"},"30 x W x H"),". "),(0,o.kt)("p",null,"As an example, if we wanted to check if there were any buildings in our image we could look at layer 3 of our one hot encoded matrix. "),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://www.jeremyjordan.me/content/images/2018/05/Screen-Shot-2018-05-16-at-9.36.00-PM.png"},"Jeremy Jordan")," has created a great illustration of this concept, shown below. "),(0,o.kt)("p",{align:"center"},(0,o.kt)("img",{width:"660",height:"400",src:"../../images/oneHotExample.png"})),(0,o.kt)("h4",{id:"combining-one-hot-with-optical-flow"},(0,o.kt)("ins",null,"Combining One Hot with Optical Flow")),(0,o.kt)("p",null,"Optical flow preserves the direction of each object. If the optical flow shows that the car is moving towards a building then when we apply matrix multiplication of optical flow and one hot matrices we preserve that movement information. Since our one hot matrix tells us when something is not in an image, when we apply matrix multiplication any optical flow movement at that area will be considered insignificant and will be rendered as 0. "),(0,o.kt)("p",null,"In the next post we'll insert these matrices into our LSTM model."))}h.isMDXComponent=!0}}]);